{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7nFCXK6zebfm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "detectron2",
      "display_name": "Detectron2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating NADA"
      ],
      "metadata": {
        "id": "v9YDl81PmB6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for evaluating NADA.\n",
        "\n",
        "Prepare the data and evaluator, load the predictions depending on the prompt construction (template or caption), and perform evaluation."
      ],
      "metadata": {
        "id": "nHgXn2K3qMCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "gJq6sgYhmE3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "from detectron2.structures import Instances, Boxes\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "from data import setup_artdl, setup_iconart\n",
        "from coco_eval import CustomIOUCOCOEvaluator"
      ],
      "metadata": {
        "id": "ekPgrRAcmGoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing data"
      ],
      "metadata": {
        "id": "LSyG8IgemIsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = ''\n",
        "# you'll  most likely in interseted in 'artdl_test' and 'iconart_test'\n",
        "# see data.py for more options\n",
        "\n",
        "setup_artdl()\n",
        "setup_iconart()\n",
        "\n",
        "dataset = DatasetCatalog.get(dataset_name)\n",
        "metadata = MetadataCatalog.get(dataset_name)"
      ],
      "metadata": {
        "id": "6B90giS8mKCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare evaluator"
      ],
      "metadata": {
        "id": "bKRW7aCdmiH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = COCOEvaluator(dataset_name, output_dir='results')\n",
        "\n",
        "# uncomment the line below if you want a custom IOU not shown by default by COCOEvaluator or if you want per-class results that aren't AP 0.50:0.95\n",
        "# evaluator = CustomIOUCOCOEvaluator(dataset_name, iou=0.5, output_dir=output_dir)"
      ],
      "metadata": {
        "id": "FZ_UZRkKmjad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading predictions"
      ],
      "metadata": {
        "id": "aurJXQJ1p4gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Template prompt construction"
      ],
      "metadata": {
        "id": "WQ7nxPeP5GpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = []\n",
        "outputs = []\n",
        "missing_preds = []\n",
        "incomplete_annotations = []\n",
        "\n",
        "all_class_preds_filepath = ''\n",
        "all_det_preds_filepath = ''\n",
        "\n",
        "for item in tqdm(dataset):\n",
        "  pred_boxes, scores, pred_classes = [], [], []\n",
        "\n",
        "  with open(os.path.join(\n",
        "      all_class_preds_filepath,\n",
        "      f\"{item['image_id']}.json\"\n",
        "  )) as f:\n",
        "    labels = json.load(f)['labels']\n",
        "\n",
        "  for label in labels:\n",
        "    label_id = metadata.thing_classes.index(label)\n",
        "\n",
        "    pred_filepath = os.path.join(\n",
        "        all_det_preds_filepath,\n",
        "        label,\n",
        "        item['image_id']+'.json'\n",
        "    )\n",
        "\n",
        "    if os.path.exists(pred_filepath):\n",
        "      annotations_exist = True\n",
        "\n",
        "      with open(pred_filepath) as f:\n",
        "        preds = json.load(f)\n",
        "        for pred_ann in preds['annotations']:\n",
        "          x, y, w, h = pred_ann['bbox']\n",
        "          pred_boxes.append([x, y, x+w, y+h])\n",
        "          scores.append(1.0)\n",
        "          pred_classes.append(pred_ann['id'])\n",
        "    else:\n",
        "      missing_preds.append(pred_filepath)\n",
        "\n",
        "  annotations_exist = (len(pred_boxes) == len(scores) == len(pred_classes)) and len(pred_boxes) > 0\n",
        "  inputs.append(item)\n",
        "  outputs.append({\n",
        "      'instances': Instances(\n",
        "          (item['height'], item['width']),\n",
        "          pred_boxes=Boxes(torch.tensor(pred_boxes)),\n",
        "          scores=torch.tensor(scores),\n",
        "          pred_classes=torch.tensor(pred_classes)\n",
        "      )\n",
        "  })\n",
        "  if not annotations_exist:\n",
        "    print(item['image_id'], labels)\n",
        "    incomplete_annotations.append(item['image_id'])"
      ],
      "metadata": {
        "id": "ex6AoSmd5LLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caption prompt construction"
      ],
      "metadata": {
        "id": "sJfv3MkvcL0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.structures import Instances, Boxes\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pprint import pprint\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "missing_preds = []\n",
        "incomplete_annotations = []\n",
        "\n",
        "all_caps_filepath = ''\n",
        "all_det_preds_filepath = ''\n",
        "\n",
        "for item in tqdm(dataset):\n",
        "  pred_boxes, scores, pred_classes = [], [], []\n",
        "\n",
        "  with open(os.path.join(\n",
        "      all_caps_filepath,\n",
        "      f\"{item['image_id']}.json\"\n",
        "  )) as f:\n",
        "    labels = [list(label_caption.keys())[0] for label_caption in json.load(f)['prompts_for_generation']]\n",
        "\n",
        "  for label in labels:\n",
        "    label_id = metadata.thing_classes.index(label)\n",
        "\n",
        "    pred_filepath = os.path.join(\n",
        "        all_det_preds_filepath,\n",
        "        label,\n",
        "        item['image_id']+'.json'\n",
        "    )\n",
        "\n",
        "    if os.path.exists(pred_filepath):\n",
        "      annotations_exist = True\n",
        "\n",
        "      with open(pred_filepath) as f:\n",
        "        preds = json.load(f)\n",
        "        for pred_ann in preds['annotations']:\n",
        "          x, y, w, h = pred_ann['bbox']\n",
        "          pred_boxes.append([x, y, x+w, y+h])\n",
        "          scores.append(1.0)\n",
        "          pred_classes.append(pred_ann['id'])\n",
        "    else:\n",
        "      missing_preds.append(pred_filepath)\n",
        "\n",
        "  annotations_exist = (len(pred_boxes) == len(scores) == len(pred_classes)) and len(pred_boxes) > 0\n",
        "  inputs.append(item)\n",
        "  outputs.append({\n",
        "      'instances': Instances(\n",
        "          (item['height'], item['width']),\n",
        "          pred_boxes=Boxes(torch.tensor(pred_boxes)),\n",
        "          scores=torch.tensor(scores),\n",
        "          pred_classes=torch.tensor(pred_classes)\n",
        "      )\n",
        "  })\n",
        "  if not annotations_exist:\n",
        "    print(item['image_id'], labels)\n",
        "    incomplete_annotations.append(item['image_id'])"
      ],
      "metadata": {
        "id": "I7pOp5w3cL0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating predictions"
      ],
      "metadata": {
        "id": "mtEgczccp_uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.reset()\n",
        "evaluator.process(inputs, outputs)\n",
        "results = evaluator.evaluate()"
      ],
      "metadata": {
        "id": "UTToeYaQcL0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}